## "Rationalist Discourse" Is Like "Physicist Motors"

Imagine being a student of physics, and coming across a blog post proposing a list of guidelines for "physicist motors"—motor designs informed by the knowledge of physicists, unlike ordinary motors.

Even if most of the things on the list seemed like sensible guidelines to keep in mind when designing a motor, the framing would seem very odd. The laws of physics describe how energy can be converted into work. To the extent that _any_ motor accomplishes anything, it happens within the laws of physics. There are theoretical ideals describing how motors need to work in principle, like [the Carnot engine](https://en.wikipedia.org/wiki/Carnot_heat_engine), but you can't actually build an ideal Carnot engine; real-world electric motors or diesel motors or jet engines all have their own idiosyncratic lore depending on the application and the materials at hand; an engineer who worked on one, might not the be best person to work on another. You might appeal to principles of physics to explain why some particular motor is inefficient or poorly-designed, but you would not speak of _physicist motors_ as if that were a distinct category of thing—and if someone _did_, you might quietly begin to doubt how much they really knew about physics.

As a student of rationality, I feel the same way about guidelines for "rationalist discourse." The laws of probability and decision theory describe how information can be converted into optimization power. To the extent that _any_ discourse accomplishes anything, [it happens within the laws of rationality](https://www.lesswrong.com/posts/eY45uCCX7DdwJ4Jha/no-one-can-exempt-you-from-rationality-s-laws).

Rob Bensinger proposes ["Elements of Rationalist Discourse"](https://www.lesswrong.com/posts/svuBpoSduzhYjFPrA/elements-of-rationalist-discourse) as a companion to Duncan Sabien's earlier ["Basics of Rationalist Discourse"](https://www.lesswrong.com/posts/XPv4sYrKnPzeJASuk/basics-of-rationalist-discourse-1). _Most_ of the things on both lists are, indeed, sensible guidelines that one might do well to keep in mind when arguing with people, but as Bensinger notes, "Probably this new version also won't match '_the_ basics' as other people perceive them."

But there's a reason for that: a list of guidelines has the wrong type signature for being "_the_ basics". The _actual_ basics are the principles of rationality one would appeal to _explain which guidelines are a good idea_: principles like how [evidence is the systematic correlation between possible states of your observations and the possible states of reality that you want to know about](https://www.lesswrong.com/posts/6s3xABaXKPdFwA3FS/what-is-evidence), how [you need evidence to locate the correct hypothesis in the space of possibilities](https://www.lesswrong.com/posts/nj8JKFoLSMEmD3RGp/how-much-evidence-does-it-take), how [the quality of your conclusion can only be improved by arguments that have the power to _change_ that conclusion](https://www.lesswrong.com/posts/34XxbRFe54FycoCDw/the-bottom-line).

Contemplating these basics, it should be clear that there's just not going to be anything like a unique style of "rationalist discourse", any more than there is a unique "physicist motor." There are theoretical ideals describing how discourse needs to work in principle, like [Bayesian reasoners with common priors exchanging probability estimates](https://en.wikipedia.org/wiki/Aumann's_agreement_theorem), but you can't actually build an ideal Bayesian reasoner. Rather, different discourse algorithms (the collective analogue of ["cognitive algorithm"](https://www.lesswrong.com/posts/HcCpvYLoSFP4iAqSz/rationality-appreciating-cognitive-algorithms)) leverage the laws of rationality to convert information into optimization in somewhat different ways, depending on the application and the population of interlocutors at hand, much as electric motors and jet engines both leverage the laws of physics to convert energy into work without being identical to each other, and with each requiring their own engineering sub-specialty to design.

Or to use [another standard metaphor](https://www.lesswrong.com/posts/teaxCFgtmCQ3E9fy8/the-martial-art-of-rationality), there's also just not going to be a unique martial art. Boxing and karate and ju-jitsu all have their own idiosyncratic lore adapted to different combat circumstances, and a master of one would easily defeat a novice at the other. One might appeal to the laws of physics and the properties of the human body to explain why some particular martial arts school was not teaching their students to fight effectively. But if some particular karate master were to brand their own lessons as the "basics" or "elements" of "martialist fighting", you might quietly begin to doubt how much actual fighting they had done: either all fighting is "martialist" fighting, or "martialist" fighting isn't actually necessary for beating someone up.

One historically important form of discourse algorithm is _debate_, and its close variant the _adversarial court system_. It works by separating interlocutors into two groups: one that searches for arguments in favor of a belief, and another that seaches for arguments against the belief. Then anyone listening to the debate can consider all the arguments to help them decide whether or not to adopt the belief. (In the _court_ variant of debate, a designated "judge" or "jury" announces a "verdict" for or against the belief, which is added to the court's [shared map](https://www.lesswrong.com/posts/9QxnfMYccz9QRgZ5z/the-costly-coordination-mechanism-of-common-knowledge), where it can be referred to in subsequent debates, or "cases.")

The enduring success and legacy of the debate algorithm can be attributed to how it circumvents a critical design flaw in individual human reasoning. (At least, that's one way of looking at it—a more complete discussion would consider how individual human reasoning capabilities _co-evolved_ with the debate algorithm.) Humans have a tendency to "rationalize"—to preferentially search for new arguments for an already-determined conclusion. Once such a conclusion has been reached, even prematurely, further invocations of the biased search process are likely to further entrench the conclusion, even when strong counterarguments exist (in regions of argument-space neglected by the biased search). The debate algorithm solves this entrenched-conclusion bug by distributing a search for arguments and counterarguments among multiple humans, [ironing out falsehoods](https://www.lesswrong.com/posts/iThwqe3yPog56ytyq/aiming-for-convergence-is-like-discouraging-betting) by pitting two biased search processes against each other. (For readers more familiar with artificial than human intelligence, [generative adversarial networks](https://en.wikipedia.org/wiki/Generative_adversarial_network) work on a similar principle.)

For all its successes, the debate algorithm also suffers from many flaws.

For example, the benefits of improved conclusions mostly accrue to previously-undecided third parties;

debate participants themselves


Really, the whole idea is so galaxy-brained that it's amazing it works at all. There's only one reality, so correct information-processing should result in everyone agreeing on the best available estimate. [We have theorems about this](https://en.wikipedia.org/wiki/Aumann's_agreement_theorem), but even without studying the proofs, the result is _obvious_. So why does the world's most popular discourse algorithm take for granted the ubiquity of _dis_-agreements? Isn't that crazy?




 * But the debate algorithm has flaws
 * You might think it's possible to do better than debate, by training "rationalists" (like physic-ists)
 * But discourse-norm guidelines don't teach the things rationalists need to know: for example, this "err on the side of" stuff in "Elements" is something you say to people who _don't_ know decision theory; the part about "reward people for good epistemic conduct" might be a good idea, but it's also manipulative, and it's confusing to put manipulative training techniques on the same list as how-to-get-the-right-answer techniques without explaining the type distinction
 * If you don't know what you're doing, it's very possible to do _worse_ than debate (if you reward people for looking cooperative and looking like they're changing their mind, that's really easy to degenerate into "I accept a belief from you if you accept a belief from me" social exchange


Links—
https://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/
argumentative theory of reasoning
we change our minds less often than we think
Don't Double-Crux With Suicide Rock



Eliezer Yudkowsky [laments](https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty):

> In the end, a lot of what people got out of all that writing I did, was not the deep object-level principles I was trying to point to—they did not really get [Bayesianism as thermodynamics](https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition), say, they did not become able to see [Bayesian structures](https://www.lesswrong.com/posts/QrhAeKBkm2WsdRYao/searching-for-bayes-structure) any time somebody sees a thing and changes their belief. What they got instead was something much more meta and general, a vague spirit of how to reason and argue, because that was what they'd spent a lot of time being exposed to over and over and over again in lots of blog posts.






------

 * As a student of rationality, I feel the same way about guidelines for "rationalist discourse". Rob's list says that he disagreed with Duncan's list. There's a reason for that! The type signature is wrong. There are laws of rationality that govern how to convert information into utility. There's a theoretical ideal that we can prove theorems about (Bayesian reasoners with common priors), but there's not an ideal discourse norm, because real world discourses face different engineering constraints. Arguing on Less Wrong is different from arguing in a court. The Law isn't about trying to get people to join your internet cult

 * If you don't trust people to understand the law, maybe you can lie-to-children them with a list of guidelines, as with how Sabien's list says its 80/20. I'm more optimistic in people's capabilities; rather than proposing my own list of guidelines, I think it's an informative excercise to look at other people's guidelines, and explain how they arise from the Law.

# Bensinger's list

 * Truth-Seeking (direct appeal to the law)
 * Non-Violence (because violence doesn't distinguish)
 * Non-Deception—assumes common interests
 * Localizability (arises from the local nature of the law)
 * note that 6 says "reward betting!"
 * 9. Goodwill arises from trying to build a discourse space out of humans; it doesn't arise out of the law

"Err on the side of" can make sense as an engineering guideline (like leaving safety margin), but understand that that comes from asymmetric payoffs; you're not actually _trying to err_.

> approach people with an attitude of informing and empowering them to do what they want.

This is a good idea because, but it's not necessarily part of _rationality_. 


https://www.lesswrong.com/posts/bkSkRwo9SRYxJMiSY/beautiful-probability
https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking


https://twitter.com/ESYudkowsky/status/1355712437006204932
> A "Physics-ist" is trying to engage in a more special human activity, hopefully productively, where they *think* about light in order to use it better.
