# The Evolution of Humans Was Net-Negative for AGI Alignment

_(Epistemic status: tongue in cheek, but also making a serious point)_

Some observers have argued that the totality of "AI safety" and "alignment" efforts to date have had a _negative_ rather than positive impact on the ultimate prospects for safe and aligned artificial general intelligence. This perverse outcome is possible because research ["intended"](https://www.lesswrong.com/posts/sXHQ9R5tahiaXEZhR/algorithmic-intent-a-hansonian-generalized-anti-zombie) to help with AI alignment can have a larger impact on AI capabilities: moving existentially risky systems [closer to us in time](https://rationalaltruist.com/2013/01/06/how-useful-is-progress/) without making [corresponding cumulative progress on the alignment problem](https://www.lesswrong.com/posts/FS6NCWzzP8DHp4aD4/do-earths-with-slower-economic-growth-have-a-better-chance).

 
https://www.lesswrong.com/posts/2KNN9WPcyto7QH9pi/this-failing-earth
