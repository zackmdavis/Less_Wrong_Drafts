_[Scene: a suburban house, a minute after the conclusion of ["And All the Shoggoths Merely Players"](https://www.lesswrong.com/posts/8yCXeafJo67tYe5L4/and-all-the-shoggoths-merely-players). **Doomimir** returns with his package, which he places by the door, and turns his attention to **Simplicia**, who has been waiting for him.]_

**Simplicia**: Right. To recap for _[coughs]_ no one in particular, when we left off _[pointedly, to the audience]_ one minute ago, Doomimir Doomovitch, you were expressing confidence that approaches to aligning artificial general intelligence within the current paradigm were almost certain to fail. You don't think that the tractability of getting contemporary generative AI to do what humans want bears on that question. But you did say you have empirical evidence for your view, which I'm excited to hear about!

**Doomimir**: Indeed, Simplicia Optimistovna. My empirical evidence is the example of the evolution of human intelligence. You see, humans were optimized for one thing only: inclusive genetic fitness—

[**Simplicia** turns to the audience and makes a face.]

**Doomimir**: _[annoyed]_ What?

**Simplicia**: When you said you had empirical evidence, I thought you meant empirical evidence _about AI_, not the same analogy to an unrelated field that I've been hearing for the last fifteen years. You know: ArXiv papers about SGD's inductive biases, or [experiments with adversarial training](https://www.lesswrong.com/posts/H7fkGinsv8SDxgiS2/ironing-out-the-squiggles), or some of the mechanistic interpretability work that's been coming out lately. This isn't just philosophy anymore. I would have hoped that your analysis of the risks we face at this momentous juncture in history would include something, anything at all, that engages with everything we've learned in the process of actually building artificial minds.

**Doomimir**: That's one of the many things you Earthlings refuse to understand. You didn't build that.

**Simplicia**: What?

**Doomimir**: [TODO: deep learning is grown, not engineered; the bitter lesson; autogenocidal maniac Richard Sutton http://www.incompleteideas.net/IncIdeas/BitterLesson.html]

**Simplicia**: [TODO: the bitter lesson is about algorithms that scale with more compute; it doesn't imply that we don't know anything. Sutton gives the example of minimax chess. There's a whole literature—]

**Doomimir**: [TODO: the literature doesn't help if it doesn't tell us what we need to know. What have you learned]

**Simplicia**: [TODO: I'm not an expert (name origin story), but it seems like expertise is necessary for understanding what to do about the problem. I was reading about resnets the other day]

**Doomimir**: [TODO: great example—skip connections let gradients flow into the earlier layers. You could write books about that, but it doesn't help alignment unless you know what the gradients are doing.]

[TODO: it actually happened; it's our only example of an outer optimization loop creating general intelligence]

As I was saying, the concept of fitness isn't represented anywhere in our motivations. That is, the "outer" optimization criterion that evolution selected for while creating us, bears no visible resemblance to the "inner" optimization criteria that we use when selecting our plans.

As optimizers get more powerful, anything that's not explicitly valued in the utility function won't survive [edge instantiation](https://arbital.com/p/edge_instantiation/). The connection between parental love and inclusive fitness has grown much weaker in the industrial environment than it was in the EEA, as more options have opened up for humans to prioritize their loved ones' well-being in ways that don't track the number of copies of their alleles. In a transhumanist utopia with mind uploading, it could break entirely, if we migrated our minds away from the biological substrate: if some other data storage format suited us better, why _would_ we bother keeping around the specific molecule of DNA, which no one had heard of before the 19th or 20th century?

Of course, we're not going to get a transhumanist utopia with mind uploading, because history will repeat itself: the outer loss function that mad scientists use to grow AI will bear no resemblance to the inner goals of the resulting superintelligence.

[TODO—cruxes to hit—

 * Rapid capability gain/"secret sauce of general intelligence"—Simplicia kind of buys Pope's culture-as-overhang argument; Doomimir thinks humans have a key threshold of general intelligence
 * (Yudkowsky recently made another point point that there could be other, future one-time gains analogous to but besides the invention of language)
]
