# E. T. Jaynes and the Assumption of Observer-Biased Goal Systems

These paragraphs from E. T. Jaynes's _Probability Theory: The Logic of Science_ (in §13.12.2, "Loss functions in human society") are _fascinating_ from the perspective of a regular reader of this website:

> We note the sharp contrast between the roles of prior probabilities and loss functions in human relations. People with similar prior proabilities get along well together, because they have about the same general view of the world and philosophy of life. People with radically different prior probabilities cannot get along—this has been the root cause of all the religious wars and most of the political repressions throughout history.
>
> Loss functions operate in just the opposite way. People with similar loss functions are after the same thing, and are in contention with each other. People with different loss functions get along well because each is willing to give something the other wants. Amicable trade or business transactions, advantageous to all, are possible only between parties with very different loss functions. We illustrated this by the example of insurance above.

(Jaynes writes in terms of [loss functions](https://en.wikipedia.org/wiki/Loss_function) for which lower values are better, whereas we more often speak of [utility functions](https://en.wikipedia.org/wiki/Utility#Utility_function) for which higher values are better, but the choice of convention doesn't matter—as long as you're _extremely_ sure which one you're using.)

The passage is fascinating because the conclusion looks so _self-evidently_ wrong from our perspective. Agents with the same goals are in contention with each other? Agents with different goals get along? _What!?_

The disagreement stems from a clash of implicit assumptions. Our prototypical agent is the superintelligent paperclip maximizer, with a utility function about the universe—specifically, the number of paperclips in it—not about itself. It doesn't care _who_ makes the paperclips. It [probably doesn't even need to trade with anyone](https://www.lesswrong.com/posts/6KzFwcDy7hsCkzJKY/the-point-of-trade).

In contrast, although _Probability Theory_ makes use of the rhetorical device of programming a robot to reason, this passage seems to suggest that Jaynes hadn't thought much about 




in places makes use of the rhetorical device of programming a robot to reason, this passage seems to suggest that Jaynes hadn't thought much about agents-in-general, and is thinking about humans. (The passage didn't _say_ "agents"—but humans don't have loss functions.)
]


https://intelligence.org/files/CFAI.pdf#page=38

In contrast, in our philosophy, you don't want agents with other utility functions to exist

(published posthumously after Jaynes's 1998 death),
