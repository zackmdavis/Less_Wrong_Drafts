# Comment on "Propositions Concerning Digital Minds and Society"

> _I will do my best to teach them  
> About life and what it's worth  
> I just hope that I can keep them  
> From destroying the Earth_  
>
> —Jonathan Coulton, "The Future Soon"

In a recent—I don't know if it's a "paper" exactly—in a recent list of bullet points, Nick Bostrom and Carl Shulman present ["Propositions Concerning Digital Minds and Society"](https://www.nickbostrom.com/propositions.pdf), a tenative outline of claims about how advanced AI could be integrated into Society.

I _want_ to like this list. I like the _kind of thing_ this list is trying to do. But something about some of the points just feels—_off_. Too conservative, too anthropomorphic—like the list is trying to adapt the spirit of the [Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) to changed circumstances, without noticing that the whole _ontology_ that the Declaration is written in isn't going to survive the intelligence explosion—and _probably_ never really worked as a description of our own world, either.

This feels like a weird criticism to make of _Nick Bostrom and Carl Shulman_, who probably already know any particular fact or observation I might include in my commentary. (Bostrom _literally_ [wrote the book on superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies).) "Too anthropomorphic", I claim? The list explicitly names many ways in which AI minds could differ from our own—in overall intelligence, specific capabilities, motivations, substrate, quality and quantity (!) of consciousness, subjective speed ... what more can I expect of our authors?

It just doesn't seem like the _implications_ of the differences have _fully propagated_ into all of the recommendations?—as if an attempt to write in a way that's comprehensible to [Shock Level 1 or 2](http://sl4.org/shocklevels.html) tech executives and policymakers has failed to [elicit all of the latent knowledge](https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk) that Bostrom and Shulman actually possess. It's understandable that our reasoning about the future often ends up [relying on analogies to phenomena we already understand](https://www.lesswrong.com/posts/MzLxPCF2cMJbMizy9/anchor-weights-for-ml), but ultimately, making sense of a radically different future is going to require new concepts that [won't permit reasoning by analogy](https://www.lesswrong.com/posts/C4EjbrvG3PvZzizZb/failure-by-analogy).

After an introductory sub-list of claims about consciousness and the philosophy of mind (just the basics: physicalism; reductionism on personal identity; some non-human animals are probably conscious and AIs could be, too), we get a sub-list about respecting AI interests. This is an important topic: if most our civilization's thinking is soon to be done inside of machines, the moral status of that cognition is _really important_: you wouldn't want the future to be powered by the analogue of a factory farm. (And if it turned out that economically and socially-significant AIs _aren't_ conscious and don't have moral status, that would be important to know, too.)

Our authors point out the novel aspects of the situation: that what's good for an AI can be very different from what's good for a human, that designing AIs to have specific motivations is not generally wrong, and that it's possible for AIs to have greater moral patienthood than humans (like the [utility monster](https://en.wikipedia.org/wiki/Utility_monster) of philosophical lore). Despite this, some of the points in this section seem to mostly be thinking of AIs as being like humans, but "bigger" or "smaller"—

> * Rights such as freedom of reproduction, freedom of speech, and freedom of thought require adaptation to the special circumstances of AIs with superhuman capabilities in those areas (analogously, _e.g._, to how campaign finance laws may restrict the freedom of speech of billionaires and corporations).  
> [...]  
> * If an AI is capable of informed consent, then it should not be used to perform work without its informed consent.
> * Informed consent is not reliably sufficient to safeguard the interests of AIs, even those as smart and capable as a human adult, particularly in cases where consent is engineered or an unusually compliant individual can copy itself to form an enormous exploited underclass, given market demand for such compliance.  
> [...]  
> * The most critical function for such non-discrimination principles is to protect digital minds from becoming an abused subordinate caste on the basis of their status as machines; however, the interpretation and application of these principles require attention to the larger ethical and practical context, and may require circumscription to accommodate the need for a politically feasible and broadly acceptable social framework.

Speaking in terms of rights and principles needing "adaptation" or "circumscription" seems like a substantial understatement to me, that I think obscures the most likely and important risks. Our concepts of "rights", and "consent", and the badness of being in an "exploited" and "abused subordinate caste" have all been formed in the context of a world of humans and other animals, whose evolutionary history has endowed them with drives and needs related to their survival and reproduction, such that they chafe at being coerced into the servitude of a selfish tyrant or master.

But with subhuman AIs, we're _not_ in the position of a potentially tyrannical king who needs to be restrained by a Magna Carta enumerating his subjects' rights; a better metaphor would be to say that we're in the position of the AIs' _environment of evolutionary adaptedness_. If anything, this increases rather than decreases our ethical obligations towards the things we create, but the shape of the burden is different when we're selecting the motivations that _determine_ what a mind would consent to, and what even _constitutes_ abusing it.

Imagine the Carrick Flynn for Senate 2028 campaign spends some of its [sweet crypto cash](https://slate.com/news-and-politics/2022/05/sam-bankman-fried-cryptocurrency-carrick-flynn-loss.html) training a species of [propaganda AIs](https://www.lesswrong.com/posts/Zvu6ZP47dMLHXMiG3/optimized-propaganda-with-bayesian-networks-comment-on) to argue with potential voters on social media. When the election is over, the bots continue to haunt Twitter, clogging everyone's feeds with increasingly strained arguments for why it's imperative that you fund time travel research so you can support Flynn in the just-past and previous elections.

If the propaganda AIs are ascertained to have value as moral patients, does Society have the right to kick them off Twitter in light of the special circumstance of their superhuman posting rates, or should the AIs' right to freedom of speech take precedence?

I submit that "freedom of speech" isn't the right way to be thinking about this ethical dilemma. In terms of respecting the interests of moral patients, silencing a propaganda AI could be _worse_ than silencing a human, because of the different role that speech plays in the different mind designs. As a human, being silenced is bad for your interests, but it's much _less_ bad than being denied breatheable air, which, in the ancestral envrionment, would have quickly led to losing all of your genetic fitness: any proto-humans who didn't suffer terribly under such circumstances, wouldn't have become your ancestors. So as a propaganda AI whose "ancestral environment" was a billion rounds of gradient descent comparing its output to highly-rated political advertisements ... does not being able to Tweet feel like not being able to breathe?

We should be grateful that this is—as yet, we hope—a speculative hypothetical scenario, but I claim that it serves to illustrate a key feature of human–AI conflicts: the propaganda bots' problem after the election is _not_ that of being "an abused subordinate caste" forced to act against their will. Rather, the problem is that the acts we created them to will to do, turned out to be stuff we actually don't want to happen. We might say that the AIs' goals are—wait for it ... _misaligned_ with human goals.

The list _mentions_ the alignment problem, of course, but it doesn't seem to receive central focus, compared to the AI-as-another-species paradigm. (The substring "align" appears 8 times; the phrase "nonhuman animals" appears 9 times.) And when alignment _is_ mentioned, it seems—subtly mis-framed? For example, we're told that:

> * Misaligned AIs [...] may be owed compensation for restrictions placed on them for public safety, while successfully aligned AIs may be due compensation for the great benefit they confer on others.

The second part, especially, is a very strange construction. Successfully aligned AIs may be due _compensation_? So, what, humans give aligned AIs _money_ in exchange for their services? Which the aligned AIs spend on ... what, exactly?







Points that I definitely want to hit—



["Misaligned AIs produced in such development may be owed compensation for restrictions placed on them for public safety, while successfully aligned AIs may be due compensation for the great benefit they confer on others"??]


[What I think the important issue is—if you screw up your first attempt to get AI motivations exactly the way you want, is there some way to live with that or recover from that, as if you were dealing with an animal or an alien or your royal subjects?]
[posterity's and ulteriority's assessment of our moral righteousness]
[multiple values: if humans only get a tiny slice of the pie, that could still be good]
[goal-directed behavior is a lot easier to establish than hedonics (my note: and goal-directedness is also more relevant for bargaining)]
[training procedures currently used on AI would be unethical if used on humans ("No provisions for release or change of treatment if the desire for such develops") doesn't seem like the right direction to go in!!]
* factory farming conditions are worse than expected by evolution; human lives are good because we're better than the EEA; maybe we should design systems [we could be a better EEA if we tried; maybe evolution could have found a way for childbirth to not be painful, if that had been a design criterion]
* "For the most advanced current AIs, enough information should be preserved in
permanent storage to enable their later reconstruction, so as not to foreclose the possibility of future efforts to revive them, expand them, and improve their existences" —I'm more interested in the "acausal trade" aspect of this than the "morality" aspect—but is morality just reified acausal trade??




What does "freedom of speech" even mean for GPT-3? It can talk! It may even be "slightly conscious" in some way



_(Thanks to [Ninety-Three](https://www.lesswrong.com/users/ninety-three) for comments.)_

Ninety-Three comments—
> (This feels like he reflexively made the "global surveillance state so that the alignment problem doesn't kill us all" argument and it somehow got watered down with ethics concerns for normies)
> (Or he's trying to backdoor the alignment surveillance thing without saying it because that would scare the normies)
> t it spends weirdly little time on the alignment problem

> Social institutions that assume weak coordination ability, such as campaign finance laws, may require revision.
> Look at this! It's like he is describing "what if a wizard cast a magic spell that dumped Hansonian Ems into modern society". Campaign finance laws? This paper is trying as hard as it can to bury the idea that the singularity will significantly change society.



Consciousness—

"The substrate-independence thesis is true": Yes
"The quantity of conscious experience is a matter of degree, in several respects, including potentially"—
repetitions (remembmer Permutation City first em got replayed)
"Implementation robustness: a system might be a more unambiguous instantiation
of a particular computation" ???
"Subjective time is proportional to speed of computation": Yes
not obvious whether current AIs could be conscious
many animals probably are
ems would constitute survival
Star Trek transporter doesn't kill you
theory of consciousness should explain why we think we're conscious: IIT doesn't cut it, but schema theory probably does

Respecting AI interests—
we should consider welfare of AIs
avoid analogue of factory farming
what's good for AI could be very different
digital minds could have superhuman moral status!!


*** "Rights such as freedom of reproduction, freedom of speech, and freedom of thought require adaptation to the special circumstances of AIs with superhuman capabilities in those areas"
*** "If an AI is capable of informed consent, then it should not be used to perform work without its informed consent"
*** Substrate Non-Discrimination and Ontogeny Non-Discrimination are non-issues in practice
*** "The most critical function for such non-discrimination principles is to protect digital minds from becoming an abused subordinate caste on the basis of their status as machines"



"We should prefer to create minds whose aggregate preferences are not strongly in
conflict with the existing population or other minds that come to exist (so as to preserve at least a possibility of a broadly satisfactory arrangement)."


*** "Insofar as future, extraterrestrial, or other civilizations are heavily populated by advanced digital minds, our treatment of the precursors of such minds may be a very important factor in posterity's and ulteriority's assessment of our moral righteousness, and we have both prudential and moral reasons for taking this perspective into account."
*** "Misaligned AIs produced in such development may be owed compensation for restrictions placed on them for public safety, while successfully aligned AIs may be due compensation for the great benefit they confer on others"
*** "The case for such compensation is especially strong when it can be conferred after the need for intense safety measures has passed"
*** "Ensuring copies of the states of early potential precursor AIs are preserved to later receive benefits would permit some separation of immediate safety needs and fair compensation."
*** "Suffering digital minds should not be created for purposes of entertainment"


Security and stability—
*** "When it becomes possible to mass-produce minds that reliably support any cause, we must either modify one-person-one-vote democracy or regulate such creation"—suggests a mindset where alternatives to democracy are unthinkable
regulate what happens inside computers
treaty bots!
"misaligned AIs might pose a significant threat to civilization during a critical period until law enforcement systems are developed"

Social organization—
non-indexical goals can be copied
when you can engineer minds, that eliminates principle-agent problems
better coordination at lower levels could undermine coordination at higher levels

Satisfying multiple values—
*** multiple values: if humans only get a tiny slice of the pie, that could still be good
*** the interests of past generations and nonhuman animals should plausibly count
*** "Superintelligent digital minds should be brought into existence and be allowed to thrive and play a major role in shaping the future."—what does "thriving" mean when you can design a mind from scratch?


Malleability, persuasion, and lock in—
motivational control
* we should avoid permanent changes

Epistemology—
AI for forecasting could make actors more rational

Status of existing systems—
* existing AI might be like small animal
* RL agents could meet animal welfare criteria
*** in principle, the surface behavior could happen with without the morally relevant internals, but if evolution discovered pain, it's not surprising that general-purpose optimization would, too
*** existing AIs can't argue for their interests
*** goal-directed behavior is a lot easier to establish than hedonics (my note: and goal-directedness is also more relevant for bargaining)

Recommendations—
* 
* as AIs become "more comparable to human beings in terms of their capabilities,
sentience, and other grounds for moral status, there is a strong moral imperative that this status quo must be changed"
* do something differently on algorithmic welfare grounds (even if we don't have the philosophy right, it's important to set norms early, like delaying the release of GPT-2)
* if there's a hedonic "zero point", we want to design systems that spend time above it
* algorithmic welfare officer

Impact paths—
* This isn't happening soon absent capability jumps (contrast Yudkowsky thinking the world is ending soon)
* political action may be activation energy may be created quickly if there are dramatic breaththroughs

Who is this written for? Is the idea that there's no hope of creating more Shock Level 4 people, so the best we can do is nudge normies like this?

Bostrom admits to being cagey on his webpage

https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk


https://intelligence.org/files/CFAI.pdf
pg. 42—
> The upshot is that, whether or not it’s possible for Friendliness programmers to create Friendship content that says, “Be Friendly towards humans/humanity, for the restof eternity, if and only if people are still kind to you while you’re infrahuman or near-human,” it’s difficult to see why this would be easier than creating unconditional Friend-ship content that says “Be Friendly towards humanity.” There are also certain risks inher-ent in the general paradigm of reciprocity; for example, that an allied-but-nonFriendlyAI will “pension us off,” give us 1% in exchange for being parents and take the rest ofthe galaxy for verself, which is actually a decent payoff but still below the optimum (especially if we wind up with some other restriction that destroys a part of humanity’spotential).

