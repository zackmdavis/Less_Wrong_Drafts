## Assume Bad Faith

I've been trying to avoid the terms "good faith" and "bad faith". I'm suspicious that most people who have picked up the phrase "bad faith" from hearing it used, don't actually know what it means—and maybe, that the thing it does mean doesn't [carve reality at the joints](https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries).

People get very touchy about bad faith accusations: they think that you should assume good faith, but that if you've determined someone is in bad faith, you shouldn't even be talking to them, that you need to exile them.

What does "bad faith" _mean_, though? It doesn't mean "with ill intent." [Following _Wikipedia_](https://en.wikipedia.org/wiki/Bad_faith), bad faith is "a sustained form of deception which consists of entertaining or pretending to entertain one set of feelings while acting as if influenced by another." The great encyclopedia goes on to provide examples: the solider who waves a flag of surrender but then fires when the enemy comes out of their trenches, the attorney who prosecutes a case she knows to be false, the representative of a company facing a labor dispute who comes to the negotiating table with no intent of compromising.

That is, bad faith is when someone's apparent reasons for doing something aren't the same as the real reasons. This is distinct from malign intent. The uniformed solider who shoots you without pretending to surrender is acting in good faith, because what you see is what you get: the man whose clothes indicate that his job is to try to kill you is, in fact, trying to kill you.

The policy of assuming good faith (and mercilessly punishing rare cases of bad faith when detected) would make sense if you lived in an honest world where what you see generally is what you get (and you wanted to keep it that way), a world where the possibility of hidden motives in everyday life wasn't a significant consideration.

On the contrary, however, I think hidden motives in everyday life are ubiquitous. As evolved creatures, we're designed to believe as it benefitted our ancestors to believe. As social animals in particular, the most beneficial belief isn't always the true one, because tricking your conspecifics into adopting a map that implies that they should benefit you is sometimes more valuable than possessing the map that reflects the territory, and the most persuasive lie is the one you believe yourself. Everyone has a story about why the outcome they want just happens to be what's best for everyone, and people mostly believe their own stories, but it's not plausible that the stories are being generated by any serious attempt to figure out what's best for everyone. [A world where people were straightforwardly trying to inform each other would look shocking and alien to us.](https://www.lesswrong.com/posts/h2Hk2c2Gp5sY4abQh/lack-of-social-grace-is-an-epistemic-virtue)

But if that's the case (and you shouldn't take my word for it), being touchy about bad faith accusations seems maladaptive. If it's normal for people's stated reasons to not be the same as the real reasons, it shouldn't be beyond the pale to think that of some particular person, nor should it necessarily entail cutting the "bad faith actor" out of public life—if only because, applied consistently, there would be no one left. Why would you trust anyone so highly as to think they never have a hidden agenda? Why would you trust yourself?

The conviction that "bad faith" is unusual contributes to a warped view of the world in which conditions of intense information warfare are rationalized as an unremarkable background fact of life. In particular, people seem to believe that persistent good faith disagreements are an ordinary phenomenon—that there's nothing strange or unusual about a supposed state of affairs in which I'm an honest seeker of truth, and you're an honest seeker of truth, and yet we end up persistently disagreeing on some matter of fact.

I claim that this supposedly ordinary state of affairs is _deeply weird_ at best, and probably just fake. _Actual_ "good faith" disagreements—those where both parties are just trying to get the right answer and there are no other hidden motives, no "something else" going on—[tend not to persist](https://www.lesswrong.com/posts/iThwqe3yPog56ytyq/aiming-for-convergence-is-like-discouraging-betting).

If this claim seems counterintuitive, you may not be considering all the everyday differences in belief that are resolved so quickly and seamlessly that we tend not to notice them as "disagreements".

Suppose you and I have been planning to go to a concert, which I think I remember being on Thursday. I ask you, "Hey, the concert is on Thursday, right?" You say, "No, I just checked the website; it's on Friday."

In this case, I _immediately_ replace my belief with yours. Why wouldn't I? We both just want the right answer to the factual question of when the concert is. With no "something else" going on, no particular incentive for me to be attached to my concert-on-Thursday belief, there's nothing stopping us from converging in one step: your just having checked the website is a more reliable source than my memory, and neither you nor the website have any reason to lie. Thus, I believe you; end of story.

Similar quick-convergence behavior is expected for more complicated questions with greater uncertainty. Suppose you and I are working on some physics problem. Both of us just want the right answer, and neither of us is a particularly better calculator than the other. I got $5 \pi$; you got $\frac{2}{5} + \pi$. As soon as I learn that you got a different answer than me, my confidence in my own answer _immediately_ plummets: if we're both equally good at math, then each of us is about as likely to have made a mistake. Until we compare calculations and work out which one of us (or both) made a mistake, I don't know _how_ you got your answer, but in terms of betting probabilities, I think you're about as likely to be right as me; it wouldn't make sense for me to believe my answer simply because it's mine.

Most disagreements of note—most disagreements people _care_ about—don't behave like the concert date or physics problems examples: people are very attached to their own answers. Sometimes, with extended argument, it's possible to get someone to change their mind or admit that the other person might be right, but with nowhere near the ease of agreeing on (probabilities of) the date of an event or the result of a calculation—from which we can infer that, in most disagreements people care about, there _is_ "something else" going on than both parties just wanting to get the right answer.


 * There are lots of candidates for what the "something else" is (maybe one is motivatedly wrong, maybe both are, maybe one party just isn't very smart), but without settling that, we can say that the equals disagreeing in good faith picture is broken
 * Even if everything you say is true, that doesn't save you from "something else" going on—you're probably selecting the facts to advance your interests
 * But admitting that you're doing that isn't part of the game! The universal practice is to come up with arguments to persuade the other guy why it's in _his_ interests to do the thing that you want
 * Inferential distance isn't a compelling explanation: in the case of the disagreeing mathematicians, you tend to assign a large probability that the other guy is right, even if you don't see how they got it.
 * Some might say: bad faith is about "intent", an "honest bias" isn't bad faith
 * From the standpoint of information transfer, the difference between bias and deception is _uninteresting_. If the apple is green, and you tell me it's red and I believe you, I end up with false beliefs about the apple. It _doesn't matter_ whether you said it was red because you were lying or because you're wearing rose-colored glasses; the input–output function is the same https://www.lesswrong.com/posts/sXHQ9R5tahiaXEZhR/algorithmic-intent-a-hansonian-generalized-anti-zombie
 * At best, conscious deception is worse than bias because it offers much more resistance: someone who's merely biased will fold when presented with a compelling argument, someone's who's consciously lying will _keep_ lying until you catch them red-handed in front of an audience with power over them
 * Given that there's usually "something else" going on, how do we go on?
 * Two strategies: stick to the object-level arguments, and full-contact psychoanalysis.
 * In most cases, I _prefer_ to stick to the object-level: just engage with the text of what people said, without addressing what you think their angle is in saying it.
 * This is the main benefit of "assume good faith" norms, but a crucial difference is that you should _not_ be assuming good faith, and you should have to pretend that you are, either: just—address the text! It's not that you shouldn't suspect hidden motives, it's that those are _off-topic_.
 * The other alternative is full-on cards-on-the-table conflict-theory psychologizing. Done well, it looks like a negotiation.
 * Ironically, the strategy that corresponds to "assume good faith" is actually less honest than admitting that the disagreement is actually a negotiation
