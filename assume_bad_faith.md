## Assume Bad Faith

I've been trying to avoid the terms "good faith" and "bad faith". I'm suspicious that most people who have picked up the phrase "bad faith" from hearing it used, don't actually know what it means—and maybe, that the thing it does mean doesn't [carve reality at the joints](https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries).

People get very touchy about bad faith accusations: they think that you should assume good faith, but that if you've determined someone is in bad faith, you shouldn't even be talking to them, that you need to exile them.

What does "bad faith" _mean_, though? It doesn't mean "with ill intent." [Following _Wikipedia_](https://en.wikipedia.org/wiki/Bad_faith), bad faith is "a sustained form of deception which consists of entertaining or pretending to entertain one set of feelings while acting as if influenced by another." The great encyclopedia goes on to provide examples: the solider who waves a flag of surrender but then fires when the enemy comes out of their trenches, the attorney who prosecutes a case she knows to be false, the representative of a company facing a labor dispute who comes to the negotiating table with no intent of compromising.

That is, bad faith is when someone's apparent reasons for doing something aren't the same as the real reasons. This is distinct from malign intent. The uniformed solider who shoots you without pretending to surrender is acting in good faith, because what you see is what you get: the man whose clothes indicate that his job is to try to kill you is, in fact, trying to kill you.

The policy of assuming good faith (and mercilessly punishing rare cases of bad faith when detected) would make sense if you lived in an honest world where what you see generally is what you get (and you wanted to keep it that way), a world where the possibility of hidden motives in everyday life wasn't a significant consideration.

On the contrary, however, I think [hidden motives in everyday life are ubiquitous](https://en.wikipedia.org/wiki/The_Elephant_in_the_Brain). As evolved creatures, we're designed to believe as it benefited our ancestors to believe. As social animals in particular, the most beneficial belief isn't always the true one, because tricking your conspecifics into adopting a map that implies that they should benefit you is sometimes more valuable than possessing the map that reflects the territory, and the most persuasive lie is the one you believe yourself. The universal human default is to come up with reasons to persuade the other party why it's in their interests to do what you want—but admitting that you're doing that _isn't part of the game_. [A world where people were straightforwardly trying to inform each other would look shocking and alien to us.](https://www.lesswrong.com/posts/h2Hk2c2Gp5sY4abQh/lack-of-social-grace-is-an-epistemic-virtue)

But if that's the case (and you shouldn't take my word for it), being touchy about bad faith accusations seems counterproductive. If it's common for people's stated reasons to not be the same as the real reasons, it shouldn't be beyond the pale to think that of some particular person, nor should it necessarily entail cutting the "bad faith actor" out of public life—if only because, applied consistently, there would be no one left. Why would you trust anyone so highly as to think they never have a hidden agenda? Why would you trust yourself?

The conviction that "bad faith" is unusual contributes to a warped view of the world in which conditions of information warfare are rationalized as an inevitable background fact of existence. In particular, people seem to believe that persistent good faith disagreements are an ordinary phenomenon—that there's nothing strange or unusual about a supposed state of affairs in which I'm an honest seeker of truth, and you're an honest seeker of truth, and yet we end up persistently disagreeing on some question of fact.

I claim that this supposedly ordinary state of affairs is _deeply weird_ at best, and probably just fake. _Actual_ "good faith" disagreements—those where both parties are just trying to get the right answer and there are no other hidden motives, no "something else" going on—[tend not to persist](https://www.lesswrong.com/posts/iThwqe3yPog56ytyq/aiming-for-convergence-is-like-discouraging-betting).

If this claim seems counterintuitive, you may not be considering all the everyday differences in belief that are resolved so quickly and seamlessly that we tend not to notice them as "disagreements".

Suppose you and I have been planning to go to a concert, which I think I remember being on Thursday. I ask you, "Hey, the concert is on Thursday, right?" You say, "No, I just checked the website; it's on Friday."

In this case, I _immediately_ replace my belief with yours. We both just want the right answer to the factual question of when the concert is. With no "something else" going on, there's nothing stopping us from converging in one step: your just having checked the website is a more reliable source than my memory, and neither you nor the website have any reason to lie. Thus, I believe you; end of story.

In cases where the true answer is uncertain, we expect similarly quick convergence in probabilistic beliefs. Suppose you and I are working on some physics problem. Both of us just want the right answer, and neither of us is particularly more skilled than the other. As soon as I learn that you got a different answer than me, my confidence in my own answer _immediately_ plummets: if we're both equally good at math, then each of us is about as likely to have made a mistake. Until we compare calculations and work out which one of us (or both) made a mistake, I think you're about as likely to be right as me, even if I don't know how you got your answer. It wouldn't make sense for me to bet money on my answer being right simply because it's mine.

Most disagreements of note—most disagreements people _care_ about—don't behave like the concert date or physics problem examples: people are very attached to "their own" answers. Sometimes, with extended argument, it's possible to get someone to change their mind or admit that the other party might be right, but with nowhere near the ease of agreeing on (probabilities of) the date of an event or the result of a calculation—from which we can infer that, in most disagreements people care about, there _is_ "something else" going on besides both parties just wanting to get the right answer.

But if there's "something else" going on in typical disagreements that look like a grudge match rather than a quick exchange of information resulting in convergence of probabilities, then the belief that persistent good faith disagreements are common would seem to be in bad faith! (Because if bad faith is "entertaining [...] one set of feelings while acting as if influenced by another", believers in persistent good faith disagreements are entertaining the feeling that both parties to such a disagreement are honest seekers of truth, but acting otherwise insofar as they anticipate seeing a grudge match rather than convergence.)

Some might reply that bad faith is about conscious intent to deceive: [honest reporting of unconsciously biased beliefs](https://slatestarcodex.com/2019/07/16/against-lie-inflation/) isn't bad faith. I've previously [expressed doubt as to how much of what we call _lying_ requires conscious deliberation](https://www.lesswrong.com/posts/bSmgPNS6MTJsunTzS/maybe-lying-doesn-t-exist#The_Optimal_Categorization_Depends_on_the_Actual_Psychology_of_Deception), but a more fundamental reply is that from the standpoint of modeling information transmission, the difference between bias and deception is _uninteresting_, usually not relevant to what probability updates the receiver of the false information should be making.

If an apple is green, and you tell me that it's red and I believe you, I end up with false beliefs about the apple. It doesn't matter whether you said it was red because you were consciously lying or because you're wearing rose-colored glasses. The input–output function is the same either way: the color you report to me doesn't depend on the color of the apple.

If I'm just trying to figure out the relationship between your reports and the state of the world (as contrasted to caring about punishing liars while letting merely biased people off the hook), the main reason to care about the difference between unconscious bias and conscious deception is that the latter puts up much stronger resistance. Someone who is merely biased will often _fold_ when presented with a sufficiently compelling counterargument (or reminded to take off their rose-colored glasses); someone who's consciously lying will _keep_ lying [(and telling ancillary lies to cover up the coverup)](https://www.lesswrong.com/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies) until you catch them red-handed in front of an audience with power over them.

Given that there's usually "something else" going on in persistent disagreements, how do we go on, if we can't rely on the assumption of good faith? I see two main strategies, [each with their own cost–benefit profile](https://www.lesswrong.com/posts/SX6wQEdGfzz7GKYvp/rationalist-discourse-is-like-physicist-motors).

One strategy is to stick the object level. Arguments can be evaluated on their merits, without addressing what the speaker's angle is in saying it (even if you think there's probably an angle). This delivers most of the benefits of "assume good faith" norms; the main difference I'm proposing is that speakers' intentions be regarded as _off-topic_ rather than presumed to be honest.

The other strategy is full-contact psychoanalysis: in addition to debating the object-level arguments, interlocutors have free reign to question each other's motives. This is difficult to pull off, which is why most people most of the time should stick to the object level. Done well, it looks like a negotiation: in the course of discussion, pseudo-disagreements (where I argue for a belief because it's in my interests for that belief to be on the shared map) are factorized out into real disagreements and bargaining over interests so that Pareto improvements can be located and taken, rather than both parties fighting to distort the shared map in the service of their interests.

For an example of what a pseudo-disagreement looks like, imagine that I own a factory that I'm considering expanding onto the neighboring wetlands, and you run a local environmental protection group. The factory emits small amounts of Examplistrene gas. You argue before the regulatory commission that the expansion should be blocked because the latest Science shows that Examplistrene makes birds sad. I counterargue that the latest–latest Science shows that Examplistrene actually makes birds happy; the previous studies misheard their laughter as tears and should be retracted.

Realistically, it seems unlikely that our apparent disagreement is "really" about the effects of Examplistrene on avian mood regulation. More likely, what's actually going on is a conflict rather than a disagreement: I want to expand my factory onto the wetlands, and you want me to not do that. The question of how Examplistrene pollution affects birds only came into it to persuade the regulatory commission.

It's inefficient that our conflict is being disguised as a disagreement. We can't both get what we want, but however the factory expansion question ultimately gets resolved, it would be better to reach that outcome without distorting Society's shared map of the bioactive properties of Examplistrene. Maybe it doesn't affect the birds at all! Whatever the true answer is, Society has a better shot at figuring it out if someone is allowed to point out your bias and mine [(because facts about which evidence gets promoted to one's attention are relevant to how one should update on that evidence)](https://www.lesswrong.com/posts/DoPo4PDjgSySquHX8/heads-i-win-tails-never-heard-of-her-or-selective-reporting).

The reason I don't think it's useful to talk about "bad faith" is because the ontology of good _vs._ bad faith isn't a great fit to either discourse strategy.

If I'm sticking to the object level, it's irrelevant: I reply to what's in the text; my suspicions about the process generating the text are out of scope.

If I'm doing full-contact psychoanalysis, the problem with "I don't think you're here in good faith" is that it's insufficiently _specific_. Rather than accusing someone of generic "bad faith", the way to move the discussion forward is by positing that one's interlocutor has some specific motive that hasn't yet been made explicit—and the way to defend oneself against such an accusation is by [making the case that one's real agenda isn't the one being proposed](http://zackmdavis.net/blog/2022/05/plea-bargaining/), rather than protesting one's "good faith" and implausibly claiming not to have an agenda.

The two strategies can be mixed. A simple meta-strategy that performs well without imposing too high of a skill requirement is to default to the object level, and only pull out psychoanalysis as a last resort against [stonewalling](https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters).

Suppose you point out that my latest reply seems to contradict something I said earlier, and I say, "Look over there, a distraction!"

If you want to continue sticking to the object level, you could say, "I don't understand how the distraction is relevant to resolving the inconsistency in your statements that I raised." On the other hand, if you want to drop down into psychoanalysis, you could say, "I think you're only pointing out the distraction because you don't want to be pinned down." Then I would be forced to either address your complaint, or explain why I had some other reason to point out the distraction.

Crucially, however, the choice of whether to investigate motives doesn't depend on an assumption that only "bad guys" have motives—as if there were bad faith actors who have an angle, and good faith actors who are ideal philosophers of perfect emptiness. There's always an angle; the question is which one.
