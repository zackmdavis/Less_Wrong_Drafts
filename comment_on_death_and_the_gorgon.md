## Comment on "Death and the Gorgon"

_(some plot spoilers)_

There's something distinctly uncomfortable about reading Greg Egan in the 2020s. Besides telling gripping tales with insightful commentary on the true nature of mind and existence, Egan stories written in the 1990s and set in the twenty-first century excelled at speculative worldbuilding, imagining what technological wonders might exist in the decades to come and how Society might adapt to them.

In contrast, "Death and the Gorgon", published in the January/February 2024 issue of _Asimov's_, feels like it's set [twenty minutes into the future](https://tvtropes.org/pmwiki/pmwiki.php/Main/TwentyMinutesIntoTheFuture). The technologies on display are an AI assistant for police officers (capable of performing research tasks and carrying on conversation) and real-time synthetic avatars (good enough to pass as a video call with a real person). When these kinds of products showed up in "'90s Egan"—I think of Worth's "pharm" custom drug dispenser in _Distress_ (1995) or Maria's "mask" for screening spam calls in _Permutation City_ (1994)—it was part of the background setting of a more technologically advanced world than our own.

Reading "Gorgon" in 2024, not only do the depicted capabilities seem less out of reach (our language model assistants and deepfakes aren't quite there yet, but don't seem too far off), but their literary function has changed: much of the moral of "Gorgon" seems to be to chide people in the real world who are overly impressed by ChatGPT. Reality and Greg Egan are starting to meet in the middle.

Our story features Beth, a standard-issue Greg Egan protagonist[^egan-protagonist] as a small-town Colorado sheriff investigating the suspicious destruction of a cryonics vault in an old mine: a naturally occurring cave-in seems unlikely, but it's not clear who would have the motive to thaw (murder?) a hundred frozen heads.

[^egan-protagonist]: Some people say that Greg Egan is bad at characterization. I think he just specializes in portraying _reasonable_ people, who don't have grotesque personality flaws to be the subject of "characterization."

Graciously tolerating the antics of her deputy, who is obsessed with the department's trial version of (what is essentially) ChatGPT-for-law-enforcement, Beth proceeds to interview the next of kin, searching for a motive. She discovers that many of the cryopreserved heads were beneficiaries of a lottery for terminally ill patients in which the prize was free cyronic suspension. The lottery is run by OG—"Optimized Giving"—a charitable group concerned with risks affecting the future of humanity. As the investigation unfolds, Beth and a colleague at the FBI begin to suspect that the lottery is a front for a creative organized crime scheme: OG is recruiting terminal patients to act as assassins, carrying out hits in exchange for "winning" the lottery. (After which another mafia group destroyed the cryonics vault as retaliation.) Intrigue, action, and a cautionary moral ensue as our heroes make use of ChatGPT-for-law-enforcement to prove their theory and catch OG red-handed before more people get hurt.

-----

So, cards on the table: this story spends a lot of wordcount satirizing a subculture that, unfortunately, I can't credibly claim not to be a part of. "Optimized Giving" is clearly a spoof on the longtermist wing of Effective Altruism—and if I'm not happy about how the "Effective Altruism" brand ate my beloved rationalism over the 2010s, I don't think anyone would deny the contiguous memetic legacy involving many of the same people. ([Human subcultures are nested fractally](https://xkcd.com/1095/); for the purposes of reviewing the story, it would benefit no one for me to to insist that Egan isn't talking about me and my people, even if, from _within_ the subculture, it looks like the OpenPhil people and the MIRI people and the Vassarites and ... _&c._ are all totally different and in fact hate each other's guts.)

I don't want to be defensive, because I'm _not_ loyal to the subculture, its leaders, or its institutions. In the story, Beth talks to a professor—think [Émile Torres](https://en.wikipedia.org/wiki/%C3%89mile_P._Torres#Transhumanism,_longtermism,_and_effective_altruism) as a standard-issue Greg Egan character—who studies "apostates" from OG who are angry about "the hubris, the deception, and the waste of money." That resonated with me a lot: I have a long [dumb](http://unremediatedgender.space/2023/Jul/blanchards-dangerous-idea-and-the-plight-of-the-lucid-crossdreamer/) [story](http://unremediatedgender.space/2023/Jul/a-hill-of-validity-in-defense-of-meaning/) [to tell](http://unremediatedgender.space/2023/Dec/if-clarity-seems-like-death-to-them/) [about hubris and deception](http://unremediatedgender.space/2024/Mar/agreeing-with-stalin-in-ways-that-exhibit-generally-rationalist-principles/), and the corrupting forces of money are probably a big part of the explanation for [the rise and predictable perversion of Effective Altruism](http://benjaminrosshoffman.com/effective-altruism-is-self-recommending/).

So if my commentary on Egan's satire contains some criticism, it's absolutely _not_ because I think my ingroup is beyond reproach and doesn't deserve to satirized. They (we) absolutely do. (I took joy in including a similar caricature in [one of my own stories](http://unremediatedgender.space/2023/Oct/fake-deeply/).) But if Egan's satire doesn't quite hit the mark of explaining exactly why the group is bad, it's not an act of partisan loyalty for me to contribute my nuanced explanation of what I think it gets right and what it gets wrong. I'm not carrying water for the movement;[^group-criticism] it's just a topic that I happen to have a lot of information about.

[^group-criticism]: I do feel bad about the fraction of my recent writing output that consists of criticizing the movement—not because it's disloyal, but because it's _boring_. I keep telling myself that one of these years I'm going to have healed enough trauma to forget about these losers already and just read ArXiv papers. Until then, you get posts like this one.

Without calling it a fair portrayal, the OG of "Gorgon" isn't a strawman conjured out of thin air; the correspondences to its real-world analogue are clear. When our heroine suspiciously observes that these _soi-disant_ world-savers don't seem to be spending anything on climate change and the Émile Torres–analogue tells her that OG don't regard it as an existential threat, [this is also true of real-world EA](https://forum.effectivealtruism.org/posts/eJPjSZKyT4tcSGfFk/climate-change-is-in-general-not-an-existential-risk). When the Torres-analogue says that "OG view any delay in spreading humanity at as close to light-speed as possible as the equivalent of murdering all the people who won't have a chance to exist in the future," the argument isn't a fictional parody; it's a somewhat uncharitably phrased summary of Nick Bostrom's ["Astronomical Waste: The Opportunity Cost of Delayed Technological Development"](https://nickbostrom.com/papers/astronomical-waste/). When the narrator describes some web forums as "interspers[ing] all their actual debunking of logical fallacies with much more tendentious claims, wrapped in cloaks of faux-objectivity" and being "especially prone to an abuse of probabilistic methods, where they pretended they could quantify both the likelihood and the potential harm for various implausible scenarios, and then treated the results of their calculations—built on numbers they'd plucked out of the air—as an unimpeachable basis for action", one could quibble with the disparaging description of subjective probability, but you can tell which website is being alluded to.

The cryonics-as-murder-payment lottery fraud is fictional, of course, but I'm inclined to read it as artistically-licensed commentary on a strain of ends-justify-the-means thinking that does exist within EA. EA organizations don't take money from the mob for facilitating contract killings, but they _did_ take money from [the largest financial fraud in history](https://en.wikipedia.org/wiki/FTX), [which was explicitly founded as a means to make money for EA](https://thezvi.wordpress.com/2023/10/24/book-review-going-infinite/). (One could point out that the charitable beneficiaries of Sam Bankman-Fried's largesse didn't know that FTX wasn't an honest business, but we have to assume that the same is true of OG in the story: only a few insiders would be running the contract murder operation, not the rank-and-file believers.)

While the depiction of OG in the story clearly shows familiarity with the source material, the satire feels somewhat lacking _qua_ anti-EA advocacy insofar as it relies too much on mere dismissal rather than presenting clear counterarguments.[^satire] The effect of OG-related web forums on a vulnerable young person are described thus:

[^satire]: On the other hand, one could argue that satire just isn't the right medium for presenting counterarguments, which would take up a lot of wordcount without advancing the story. Not every written work can accomplish all goals! Maybe it's fine for this story to make fun of the grandiose and cultish elements within longtermist EA (and there are a lot of them), with a critical evaluation of the ideas being left to other work. But insofar as the goal of "Gorgon" is to persuade readers that the ideas aren't even worthy of consideration, I think that's a mistake.

> Super-intelligent AIs conquering the world; the whole Universe turning out to be a simulation; humanity annihilated by aliens because we failed to colonize the galaxy in time. Even if it was all just stale clichés from fifty-year-old science fiction, a bright teenager like Anna could have found some entertainment value analyzing the possibilities rigorously and puncturing the forums' credulous consensus. But while she'd started out healthily skeptical, some combination of in-forum peer pressure, the phony gravitas of trillions of future deaths averted, and the corrosive effect of an endless barrage of inane slogans pimped up as profound insights—all taking the form "X is the mind-killer," where X was pretty much anything that might challenge the delusions of the cult—seemed to have worn down her resistance in the end.

I absolutely agree that healthy skepticism is critical when evaluating ideas and that in-forum peer pressure and the gravitas of a cause (for any given set of peers and any given cause) are troubling sources of potential bias—and that just because a group pays lip service to the value of healthy skepticism and the dangers of peer pressure and gravitas, doesn't mean the group's culture isn't still falling prey to the usual dysfunctions of groupthink. (As the inane slogan goes, ["Every cause wants to be a cult."](https://www.lesswrong.com/posts/yEjaj7PWacno5EvWa/every-cause-wants-to-be-a-cult))

That being said, however, ideas ultimately need to be judged on their merits, and the narration in this passage[^this-passage] isn't giving the reader any counterarguments to the ideas being alluded to. (As Egan would know, science fiction authors having written about an idea does not make the idea false.) The clause about the whole Universe turning out to be a simulation is probably a reference to Bostrom's [simulation argument](https://simulation-argument.com/simulation/), which is a disjunctive, conditional claim: given some assumptions in the philosophy of mind and the theory of anthropic reasoning, then _if_ future civilization could run simulations of its ancestors, then _either_ they won't want to, _or_ we're probably in one of the simulations (because there are more simulated than "real" histories). The clause about humanity being annihilated by failing to colonize the galaxy in time is probably a reference to Robin Hanson _et al._'s [grabby aliens thesis](https://grabbyaliens.com/), that the Fermi paradox can be explained by a selection effect: there's a relatively narrow range of parameters in which we would see signs of an expanding alien civilization in our skies without already having been engulfed by them.

[^this-passage]: In critically examining this passage, I don't want to suggest that "Gorgon"'s engagement with longtermist ideas is all snark and no substance. Earlier in the story, Beth compares OG believers "imagin[ing] that they're in control of how much happiness there'll be in the next trillion years" to a child's fantasy of violating relativity by twirling a rope millions of miles long. That's substantive: even if the future of humanity is very large, the claim that a nonprofit organization today is in a position to meaningfully affect it is surprising and should not be accepted uncritically on the basis of [evocative storytelling about the astronomical stakes](https://www.lesswrong.com/posts/pGvyqAQw6yqTjpKf4/the-gift-we-give-to-tomorrow).

No doubt many important criticisms could be made of Bostrom's or Hanson's work, perhaps by a bright teenager finding entertainment value in analyzing the possibilities rigorously. But there's an important difference between having such a criticism[^criticism-upvoted] and merely asserting that it could exist. Speaking only to my own understanding, Hanson's and Bostrom's arguments both look reasonable to me? It's certainly possible I've just been hoodwinked by the cult, but if so, the narrator of "Gorgon"'s snarky description isn't helping me snap out of it.

[^criticism-upvoted]: Which I think would get upvoted on this website if it were well done—certainly if it were written with the insight and rigor characteristic of a standard-issue Greg Egan protagonist.

It's worth noting that despite the notability of Hanson's and Bostrom's work, in practice, I don't see anyone in the subculture particularly worrying about losing out on galaxies due to competition with aliens—admittedly, because we're worried about "super-intelligent AIs conquering the world" first.[^reduce-xrisk] About which, "Gorgon" ends on a line from Beth about "the epic struggle to make computers competent enough to help bring down the fools who believe that they're going to be omnipotent."

This is an odd take from the author[^from-the-author] of [multiple](https://gregegan.net/DIASPORA/DIASPORA.html) [novels](https://www.gregegan.net/SCHILD/SCHILD.html) in which software minds engage in astronomical-scale engineering projects. Accepting the premise that institutional longtermist EA deserves condemnation for being goofy and a fraud: in condemning them, why single out as the characteristic belief of this despicable group, the idea that future AI could be really powerful?[^omnipotent] Isn't that at least credible? Even if you think people in the cult or who work at AI companies are liars or dupes, it's harder to say that about eminent academics like Stuart Russell, Geoffrey Hinton, Yoshua Bengio, David Chalmers, and Daniel Dennett, who signed [a statement affirming that "[m]itigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."](https://www.safe.ai/work/statement-on-ai-risk)[^cais-statement]

[^reduce-xrisk]: Bostrom's "Astronomical Waste" concludes that "The Chief Goal for Utilitarians Should Be to Reduce Existential Risk": making sure colonization happens at all (by humanity or worthy [rather than unworthy](https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer) successors) is more important that making it happen faster.

[^from-the-author]: In context, it seems reasonable to infer that Beth's statement is author-endorsed, even if fictional characters do not in general represent the author's views.

[^omnipotent]: I'm construing "omnipotent" as rhetorical hyperbole; influential subcultural figures [clarifying that no one thinks superintelligence will be able to break the laws of physics](https://x.com/ESYudkowsky/status/1658616828741160960) seems unlikely to be exculpatory in Egan's eyes.

[^cais-statement]: Okay, the drafting and circulation of the statement by Dan Hendrycks's [Center for AI Safety](https://www.safe.ai/) was arguably cult activity. (While Hendrycks has a PhD from UC Berkeley and [co-pioneered the usage of a popular neural network activation function](https://arxiv.org/abs/1606.08415), he [admits that his career focus on AI safety was influenced by](https://archive.ph/20230708182452/https://www.bostonglobe.com/2023/07/06/opinion/ai-safety-human-extinction-dan-hendrycks-cais/#selection-1909.0-1913.10) the EA advice-counseling organization [80,000 hours](https://80000hours.org/). But Russell, Hinton, _et al_. did sign.

Egan's own work sometimes features artificial minds with goals at odds with their creator, as in ["Steve Fever"](https://www.technologyreview.com/2007/10/15/223446/steve-fever/) (2007) or ["Crystal Nights"](https://gregegan.net/MISC/CRYSTAL/Crystal.html) (2008), and with substantial advantages over biological creatures: in _Diaspora_ (1997), the polis citizens running at 800 times human speed were peace-loving, but surely could have glassed the fleshers in a war if they wanted to. If you believe that AI could be at odds with its creators and hold a competitive advantage, scenarios along the lines of "super-intelligent AIs conquering the world" should seem plausible rather than far-fetched—a natural phenomenon straightforwardly analogous to human empires conquering other countries, or humans dominating other animals.

Given so many shared premises, it's puzzling to me why Egan seems to have to bear so much antipathy towards "us",[^historical-antipathy] rather than than regarding the subculture more coolly, as a loose amalgamation of people interested in many of the same topics as him, but having come to somewhat different beliefs. (Egan doesn't seem to think human-level AI is at all close, nor that AI could be qualitatively superhumanly intelligent; an aside in _Schild's Ladder_ (2002) alludes to a fictional result that there's nothing "above" general intelligence of the type humans have, _modulo_ speed and memory.) He seems to expect the feeling to be mutual: when someone remarked on Twitter about finding it funny that the _Less Wrong_ crowd likes his books, Egan [replied](https://twitter.com/gregeganSF/status/1727940487255138404), "Oh, I think they've noticed, but some of them still like the, err, 'early, funny ones' that predate the cult and hence devote no time to mocking it."

[^historical-antipathy]: This isn't the first time Egan has satirized the memetic lineage that became longtermist EA; _Zendegi_ (2010) [features negative portrayals of](https://www.overcomingbias.com/p/egans-zendegihtml) a character who blogs at _overpoweringfalsehood.com_ (a reference to [_Overcoming Bias_](https://www.overcomingbias.com/)) and a Benign Superintelligence Bootstrap Project (a reference to what was then the Singularity Institute for Artificial Intelligence).

Well, I can't speak for anyone else, but personally, _I_ like Egan's later work, including "Death and the Gorgon."[^early-egan] Why wouldn't I? I am not so petty as to let my appreciation of well-written fiction be dulled by the incidental fact that I happen to disagree with some of the author's views on artificial intelligence and a social group that I can't credibly claim not to be a part of. That kind of dogmatism would be contrary to the ethos of humanism and clear thinking that I learned from reading Greg Egan and _Less Wrong_—an ethos that doesn't endorse blind loyalty to every author or group you learned something from, but a discerning loyalty to whatever was _good_ in what the author or group saw in our shared universe. I don't know what the future holds in store for humanity. But whatever risks and opportunities nature may present, I think our odds are better for every thinking individual who tries to read widely and see more.[^hanson-egan]

[^early-egan]: Okay, I should confess that I do treasure early Egan (_Quarantine_ (1992)/_Permutation City_ (1994)/_Distress_ (1995)) more than later Egan, but not because they devote no time to mocking the cult. It's because I'm not smart enough to properly appreciate all the alternate physics in, _e.g._, _Schild's Ladder_ (2002) or the _Orthogonal_ trilogy (2011–2013).

[^hanson-egan]: Though we're [unlikely to get it](https://twitter.com/robinhanson/status/1365662127504187396), I've sometimes wished for a Greg Egan–Robin Hanson collaboration; I think Egan's masterful understanding of the physical world and Hanson's unsentimental analysis of the social world would complement each other well.
