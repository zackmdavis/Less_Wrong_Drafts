## Aiming for Convergence Is Like Discouraging Betting

### Summary

 * In [a list of guidelines for rational discourse](https://www.lesswrong.com/posts/XPv4sYrKnPzeJASuk/basics-of-rationalist-discourse-1), Duncan Sabien proposes that one should "[a]im for convergence on truth, and behave as if your interlocutors are also aiming for convergence on truth."

 * However, prediction markets illustrate fundamental reasons why rational discourse doesn't particularly look like "aiming for convergence". Market prices converge on truth _because_ traders can only make money by looking for divergences where their beliefs are more accurate than the market's.

 * If ["bad faith"](https://en.wikipedia.org/wiki/Bad_faith) means putting on an appearance of being moved by one set of motives, while actually acting from another, then a guideline to "behave as if your interlocutors are also aiming for convergence on truth" would seem to be an instruction to behave in bad faith.

-----

Mostly, I don't expect to disagree with heavily-traded prediction markets. If the market says it's going to rain on Saturday with 85% probability, then I (lacking any special meterology knowledge) basically think it's going to rain on Saturday at 85% probability.

Why is this? Why do I defer to the market, instead of tarot cards, or divination sticks, or my friend Maddie the meterology enthusiast?

Well, I don't expect the tarot cards to tell me anything about whether it will rain on Saturday, because there's no [plausible physical mechanism by which information about the weather could influence the cards](https://www.lesswrong.com/posts/6s3xABaXKPdFwA3FS/what-is-evidence). Shuffling and dealing the cards should work the same in worlds where it will rain and worlds where it won't rain. Even if there is _some_ influence (because whether it will rain affects the moisture and atmospheric pressure in the air, which affects my grip on the cards, which affects my shuffling motion?), it's not something I can detect from which cards are drawn.

I _do_ expect my friend Maddie the meterology enthusiast to tell me something about whether it will rain on Saturday. That's because she's always looking at the latest satellite cloud data and tinkering with her computer models, which is a mechanism by which information about the weather can influence her forecasts. The cloud data will be different in worlds where it will rain and worlds where it won't rain. If Maddie is pretty sharp and knows her stuff, maybe she can tell the difference.

And yet—no offense, Maddie—I expect the market to do even better. It's not just that the market has a lot of other pretty sharp people looking at the cloud data, and that maybe some of them are even sharper than Maddie, even though Maddie is my friend and my friends are the best.

It's that the market mechanism _rewards people for being [less wrong](https://tvtropes.org/pmwiki/pmwiki.php/Main/TitleDrop) than the market_. If the rain-on-Saturday market is trading at 85%, and Maddie's rival Kimber buys 100 shares of No, that doesn't mean Kimber thinks it's not going to rain. It means Kimber thinks 85% is _too high_. If Kimber thinks it's ["actually"](https://www.lesswrong.com/posts/f6ZLxEWaankRZ2Crv/probability-is-in-the-mind) only going to rain with 80% probability, then she figures that a No share that pays out $1 if it doesn't rain should be worth 20¢. If it's currently trading for 15¢, it's worth buying for the ["expected"](https://en.wikipedia.org/wiki/Expected_value) profit of 5¢ per share—effectively, buying a dollar for 15¢ in the 20% of worlds where it doesn't rain—even though it's still _probably_ going to rain. If she were risk-neutral and had enough money, Kimber would have an incentive to _keep_ buying No shares from anyone willing to sell them for less than 20¢, until there were no such sellers left—at which point, the rain-on-Saturday market would be trading at 80%.

Conversely, if I can't tell whether 85% is too low or too high, then I can't expect to make money by buying Yes or No shares. There's no point in buying a dollar for 85¢ in 85% of worlds, or for 15¢ in 15% of worlds.

That's why I defer to the market. It's not that I'm aiming to converge my beliefs with those of market participants. It's not that market participants are trying to converge with each other, "cooperating" in some "collaborative truth-seeking" project. The market converges on truth (if it does) because market participants are _trying to make money off each other_, and it's not so easy to make money off of an aggregate of sharp people who are already trying to do the same. I would prefer to correctly diverge from the market—to get something right that the market is getting wrong, and make lots of money in the future when my predictions come true. But mostly, I don't know how.

-------

Unfortunately, not everything can be the subject of a prediction market. Prediction markets [work on future publicly observable measurements](https://forum.effectivealtruism.org/posts/8c7LycgtkypkgYjZx/agi-and-the-emh-markets-are-not-expecting-aligned-or?commentId=u8c7bbqtZSf2a9W6t). We bet today on whether it will rain on Saturday (which no one can be certain about), expecting to _resolve_ the bets on Saturday (when anyone can just look outside).

Most disputes of intellectual interest aren't like this. We _do_ want to know whether Britain's coal reserves were a major cause of the Industrial Revolution, or whether Greg Egan's later work has discarded the human factor for mathematical austerity, but we can't bet without some operationalization for how to settle the bet, which is lacking in cases like these that require an element of "subjective" judgement.

Nevertheless, many of the principles regarding prediction markets and when to bet in them, approximately generalize to the older social technology of debates and when to enter them.

Mostly, I don't expect to enter heavily-argued debates. If prevailing opinion on the economic history subreddit says that Britain's coal reserves were a major cause of the Industrial Revolution, then I (lacking any special economic history knowledge) basically think that Britain's coal reserves were a major cause of the Industrial Revolution.

If Kimber's sister Gertrude leaves a comment pointing to data that [cities closer to coalfields started growing faster in 1750](https://academic.oup.com/ej/article/131/635/1135/5955447), it's not because that comment constitutes the whole of Gertrude's beliefs about the causes of the Industrial Revolution. It means that Gertrude thinks that the city-growth/coal-proximity correlation is an important consideration that the discussion hadn't already taken into account; she figures that she can win status and esteem from her fellow economic history buffs by mentioning it.

Conversely, if I don't know anything about economic history, then I can't expect to win status or esteem by writing "pro-coal" or "anti-coal" comments: there's no point in saying something that's already been said upthread, or that anyone can tell I just looked up on _Wikipedia_.

That's why I defer to the forum—because (hopefully) the forum socially rewards people for being less wrong than the existing discussion. The debate converges on truth (if it does) because debaters are _trying to prove each other wrong_, and it's not so easy to prove wrong an aggregate of sharp people who are already trying to do the same.

------

In a reference post on ["Basics of Rationalist Discourse"](https://www.lesswrong.com/posts/XPv4sYrKnPzeJASuk/basics-of-rationalist-discourse-1), Duncan Sabien proposes eleven guidelines for good discussions, of which the [(zero-indexed)](https://en.wikipedia.org/wiki/Zero-based_numbering) fifth is, "Aim for convergence on truth, and behave as if your interlocutors are also aiming for convergence on truth."

This advice seems ... odd. What's this "convergence" thing about, that differentiates this guideline from "aim for truth"?

Imagine giving the analogous advice to a prediction market user: "Aim for convergence on the correct probability, and behave as if your fellow traders are also aiming for convergence on the correct probability."

In _some_ sense, this is kind of unobjectionable: you do want to make trades that bring the market price closer to your subjective probability, and in the process, you should take into account that other traders are also already doing this.

But interpreted another way, the advice is backwards: traders make money by finding _divergences_ where their own beliefs are more accurate than the market's. The markets in which your fellow traders are aiming for convergence on the correct probability are precisely the markets that you shouldn't trade in.

(This is with respect to the sense of "aiming" in which an archer "aiming" an arrow at a target might not hit it every time, but we say that their "aim" is good insofar as they _systematically_ tend to hit the target, that any misses are best modeled by a random error term that can't be predicted. Similarly, the market might not always be right, but if you can _predict_ when the market is wrong, the traders must not have been "aiming" correctly from your perspective.)

Thus, every trade is an expression of the belief that your counterparty is _not_ aiming to converge on the correct probability—that there's a sucker at every table, and that _this time it isn't you_.

So why is the advice "behave as if your interlocutors are also aiming for convergence on truth", rather than "seek out conversations where you don't think your interlocutors are aiming to converge on truth, because those are exactly the conversations where you have something substantive to say instead of already having converged"?

(For example, the reason I'm writing a blog post contesting Sabien's Fifth Guideline of "Aim for convergence on truth [...]" and not the First Guideline of "Don't say straightforwardly false things", is because I think the Fifth Guideline is importantly wrong, and the First Guideline seems fine.)

Sabien's guidelines are explicitly [disclaimed to be shorthand](https://www.lesswrong.com/posts/XPv4sYrKnPzeJASuk/basics-of-rationalist-discourse-1#Prelude__On_Shorthand) that it [sometimes makes sense to violate](https://www.lesswrong.com/posts/XPv4sYrKnPzeJASuk/basics-of-rationalist-discourse-1#What_does_it_mean_for_something_to_be_a__guideline__); the post helpfully includes another 900 words elaborating on how the Fifth Guideline should be interpreted. Unfortunately, the additional exposition does not seem to clarify matters.

Sabien writes:

> If you are moving closer to truth—if you are seeking available information and updating on it to the best of your ability—then you will inevitably eventually move closer and closer to agreement _with all the other agents who are also seeking truth_.

But this can't be right. To see why, substitute "making money on prediction markets" for "moving closer to truth" and "trying to make money on prediction markets" for "seeking truth." But the only way to make money on prediction markets is by correcting mispricings, which necessarily entails moving away from agreement from the current consensus. ([As it is written](https://www.lesswrong.com/posts/wCqfCLs8z5Qw4GbKS/the-importance-of-saying-oops), not every change is an improvement, but every improvement is necessarily a change.)

Perhaps the crux is about what it means to "aim" for convergence (separately from whether actually one does converge), to "seek" truth (separately from actually getting the right answer)? I imagine some readers being baffled at how casually I move from "the conversation/market is predictably wrong" to "the traders/interlocutors aren't trying to converge on truth"? I have to be playing dumb here, right? Surely I must know the difference between bad faith and merely being honestly biased?

I'm ... [not sure I do](https://www.lesswrong.com/posts/sXHQ9R5tahiaXEZhR/algorithmic-intent-a-hansonian-generalized-anti-zombie)? Not in a _principled_ way. As a human, I certainly have an intuitive sense like anyone else that "consciously lying" is different from "rationalizing". But when I think about [computer programs that process signals construed as "evidence" to compute outputs construed as "decisions"](https://www.lesswrong.com/posts/4hLcbXaqudM9wSeor/philosophy-in-the-darkest-timeline-basics-of-the-evolution), it seems like a distinction without a difference: [if a program selectively disregards signals (evidence) that would otherwise bear on its output (decision)](https://www.lesswrong.com/posts/fmA2GJwZzYtkrAKYJ/algorithms-of-deception), it's hard to see why I should care whether the conditional statement that does it is "conscious", whatever that means. And despite the intuitive sense, I [don't think the distinction is that clear-cut in humans, either](https://www.lesswrong.com/posts/bSmgPNS6MTJsunTzS/maybe-lying-doesn-t-exist#The_Optimal_Categorization_Depends_on_the_Actual_Psychology_of_Deception).

Sabien's exposition of the Fifth Guideline seems very concerned about how to distinguish "genuine bad faith" from "good faith and genuinely trying to cooperate", about the prevalence of "defection strategies" getting in the way of "treat[ing] someone as a collaborative truth-seeker". And just—you probably don't believe me, but I feel like I just [_don't know what any of that means_](https://www.lesswrong.com/posts/uvqd3YiBcrPxXzxQM/what-does-the-word-collaborative-mean-in-the-phrase). Or rather, I know how these words map onto concepts in _my_ ontology, but that doesn't seem consistent with the way Sabien seems to be using the words.

In my vocabulary, I understand the phrase "bad faith" to mean [putting on the appearance of being moved by one set of motives, while actually acting from another](https://en.wikipedia.org/wiki/Bad_faith).

But under this definition, the Fifth Guideline seems to be an instruction to act in bad faith: we are told to "behave _as if_ [emphasis mine] your interlocutors are also aiming for convergence on truth".

Sabien gives this example of a request one might make of someone whose comments are insufficiently adhering to the Fifth Guideline:

> "Hey, sorry for the weirdly blunt request, but: I get the sense that you're not treating me as a cooperative partner in this conversation. Is, uh. Is that true?"

Suppose someone were to reply:

> "You don't need to apologize for being blunt! Let me be equally blunt. The sense you're getting is accurate: no, I am not treating you as a cooperative partner in this conversation. I think your position is wrong, and I think I know why, and I feel very motivated to explain it to you in public, partially for the education of third parties, and partially to raise my status at the expense of yours."

I consider this a _good faith_ reply. It's certainly not a polite thing to say. But _politeness is bad faith_.

In slogan form, you could perhaps say that I don't believe in collaborative truth-seeking; I believe in competitive truth-seeking. But I don't like that slogan, because in my ontology, they're not actually different things. "Attacking your argument because it sucks" sounds mean, and "Suggesting improvements to your argument to make it even better" sounds nice, but the nice/mean dimension is _not intellectually substantive_. The math is the same either way.
