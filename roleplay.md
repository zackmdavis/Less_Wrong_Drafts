## Roleplay: a Debate in One Act

**Doomer**: Humanity has made no progress on the alignment problem. Not only do we have no clue how to align a powerful optimizer to our "true" values, we don't even know how to make AI "corrigible"—willing to let us correct it. Meanwhile, capabilities continue to advance by leaps and bounds. All is lost.

**Simplicia**: Why, Doomer Doomovitch, you're such a sourpuss! It should be clear by now that advances in "alignment"—getting machines to behave in accordance with human values and intent—aren't cleanly separable from the "capabilities" advances you decry. Indeed, here's an example of GPT-4 being corrigible to me just now in the OpenAI Playground:

![](gpt-4_corrigibility.png)

**Doomer**: Simplicia Optimistovna, you cannot be serious!

**Simplicia**: Why not?

**Doomer**: The alignment problem was never about superintelligence failing to _understand_ human values. [The genie knows, but doesn't care.](https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care) The fact that a large language model trained to predict natural language text can generate that dialogue, has no bearing on the AI's actual motivations, even if the dialogue is written in the first person and notionally "about" a corrigible AI assistant character. It's just roleplay.

**Simplicia**: As you say, Doomer Doomovitch. It's just roleplay, a simulation. But _a simulation of an agent is an agent._ When we get LLMs to do cognitive work for us, the work that gets done is a matter of the LLM predicting the reasoning steps that a human would use. Conversely, when LLMs fail to faithfully mimic humans—[getting caught in a repetition trap](https://gwern.net/gpt-3#repetitiondivergence-sampling), for example—they also fail to do anything useful.

If you look at the recently touted successes of language model agents, you'll see that this is true.

[TODO—
 * chain of thought
 * https://say-can.github.io/
 * https://voyager.minedojo.org/
]

[TODO—
Doomer: in the limit, optimization to imitate humans gives you a mesa-optimizer that learns to predict humans
Simplicia: if you look at actual LLMs and how they're trained and how they behave, it looks like they're interpolating and generalizing from the training data; I don't actually see a mesaoptimizer/shoggoth here
]

[TODO—
Doomer: "Instrumental Convergence", &c. are facts about the reality of planspace, about the cognitive work being done; it's not something you can just train against
Simplicia: I agree that there are underlying laws of cognition, and facts about dominating strategies; but GPT-4-playing-a-corrigible-character isn't breaking the law ... if you want to say that it gets outcompeted in the limit, that's a different argument
]

[TODO—
https://twitter.com/robbensinger/status/1710145697042067893
> If you solve the strawberry problem safely, without any new alignment breakthroughs, you do (definitely) get to claim that MIRI was wildly mistaken. (Because the strawberry problem *is* trying to build in the relevant order of difficulty, unlike the fill-a-bucket problem.)

(when citing Rob and MIRI, have Doomer say "Never heard of them")

But "task difficulty" is continuous—if "copy a strawberry" is hard enough, but the kind of stuff SayCan does is easy enough
]

**Simplicia**: So then the fate of the lightcone depends on—

**Doomer**: I'm afraid so—

**Doomer** and **Simplicia**: [_turning to the audience_] The broader AI community figuring out which one of us is right?

**Doomer**: We're hosed.
