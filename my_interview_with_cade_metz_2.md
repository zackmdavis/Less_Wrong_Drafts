# My Interview With Cade Metz on His Reporting About Lighthaven

On 12 August 2025, I sat down with _New York Times_ reporter Cade Metz to discuss some criticisms of his 4 August 2025 article, ["The Rise of Silicon Valley's Techno-Religion"](http://archive.today/2025.08.06-024919/https://www.nytimes.com/2025/08/04/technology/rationalists-ai-lighthaven.html). The transcript below has been edited for clarity.

-----

**ZMD**: In accordance with our meetings being on the record in both directions, I have some more questions for you.

I did not really have high expectations about the August 4th article on Lighthaven and the Secular Solstice. The article is actually a little bit worse than I expected, in that you seem to be pushing a "rationalism as religion" angle really hard in a way that seems inappropriately editorializing for a news article.

For example, you write, quote,

> Whether they are right or wrong in their near-religious concerns about A.I., the tech industry is reckoning with their beliefs.

End quote. What is the word "near-religious" doing in that sentence? You could have just cut the word and just said, "their concerns about AI", and it would be a perfectly informative sentence.

**CM**: My job is to explain to people what is going on. These are laypeople. They don't necessarily have any experience with the tech industry or with your community or with what's going on here. You have to make a lot of decisions in order to do that, right? The job is to take information from lots and lots and lots of people who each bring something to the article, and then you consolidate that into a piece that tries to convey all that information. If you write a article about Google, Google is not necessarily going to agree with every word in the article.

**ZMD**: Right, I definitely understand that part. I'm definitely not demanding a puff piece about these people who I have also written critically about. But just in terms of like—

**CM**: But you and so many others in the community, who have been in the community for decades in some cases, years, use the same language. They use stronger language.

**ZMD**: Right, but so like—I'm just saying in terms of writing a news article, when you're trying to convey the who-what-when-where-why, "their concerns about AI", just with no adjective between "their" and "concerns", is much more straightforward.

**CM**: No, but people need to understand, okay, that the technology as it exists today, it is not clear how it would get to the point where it's going to, you know, destroy humanity, for instance. That is a belief that the average person doesn't understand. And when someone says that, they take it at face value, like ChatGPT is going to jump out of—

**ZMD**: That's not actually the argument, though.

**CM**: That's what I'm saying. People need to understand that the argument on some level, and people are going to debate this for years and years, or however long it takes, but that's a leap of faith, right?

**ZMD**: Yeah, that was actually my second question here. I was a little bit disappointed by the article, but the audio commentary was kind of worse. You open the audio commentary with:

> We have arrived at a moment when many in Silicon Valley are saying that artificial intelligence will soon match the powers of the human brain, even though we have no hard evidence that will happen. It's an argument based on faith.

End quote. And just, these people have written hundreds of thousands of words carefully arguing why they think powerful AI is possible and plausibly coming soon.

**CM**: That's an argument.

**ZMD**: Right.

**CM**: It's an argument.

**ZMD**: Right. 

**CM**: We don't know how to get there.

**ZMD**: Right.

**CM**: We do not—we don't know—

**ZMD**: But do you understand the difference between "uncertain probabilistic argument" and "leap of faith"? Like these are different things.

**CM**: I didn't say that. People need to understand that we don't know how to get there. There are trend lines that people see. There are arguments that people make. But we don't know how to get there. And people are saying it's going to happen in a year or two, when they don't know how to get there. There's a gap.

**ZMD**: Yes.

**CM**: And boiling this down in straightforward language for people, that's my job.

**ZMD**: Yeah, so I think we agree that we don't know how to get there. There are these arguments, and, you know, you might disagree with those arguments, and that's fine. You might quote relevant experts who disagree, and that's fine. You might think these people are being dishonest or self-deluding, and that's fine. But to call it "an argument based on faith" is different from those three things. What is your response to that?

**CM**: I've given my response.

**ZMD**: It doesn't seem like a very ...

**CM**: We're just saying the same thing.

**ZMD**: Yeah, but like I feel like there should be some way to break the deadlock of, we're just saying the same thing, like some—right?

**CM**: I feel like there should be a way to break lots of deadlocks, right?

**ZMD**: Because, for example, the Model Evaluation and Threat Research, METR, which was spun out of Paul Christiano's Alignment Research Center, they've been [measuring what tasks AI models can successfully complete in terms of how long it would take a human to complete the task](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/), and they're finding that the task length is doubling every seven months, with the idea being that when you have AI that can do intellectual tasks that would take humans a day, a week, a month, that could pose [a threat in terms of its ability to autonomously self-replicate](https://metr.org/blog/2024-11-12-rogue-replication-threat-model/). Again, you might disagree with the methodology, but that seven month doubling time thing, which is one of the things people are looking at when they're writing these wild-sounding scenarios, that's empirical work on the existing technology.

**CM**: The same lab has also released [a study saying that these LLMs actually slow coders down](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/).

**ZMD**: Yeah, I saw that, too.

**CM**:  Again, trend lines, okay, sometimes they slow down. Sometimes they stop. And these trend lines are also, they're logarithmic, they're not exponential.

**ZMD**: That's an important point, yeah.

**CM**: We agree on all this stuff.

**ZMD**: We agree on all this—

**CM**: It's about disagreeing about the best way to convey this to a person.

**ZMD**: I feel like if you said the thing you just said in the article, that would have been great. But the thing you said in the audio commentary was "an argument based on faith", which is not what you just said to me. Those are different things.

Also, speaking of this religion angle, in your previous book, _Genius Makers_, you told the story of Turing Award–winning deep learning pioneer Geoffrey Hinton. More recently, in 2023, Hinton left his position at Google specifically in order to speak up about AI risks. There was actually a nice article about this in _The New York Times_, ["'The Godfather of A.I.' Leaves Google and Warns of Danger Ahead"](http://archive.today/2024.05.27-045343/https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html). You might have seen that one.

**CM**: Yes, I did. I broke that story.

**ZMD**: That was the joke.

**CM**: I'm with you.

**ZMD**: But so Hinton [has said that his independent impression of the existential threat is more than 50%](https://www.youtube.com/watch?v=PTF5Up1hMhw&t=2283s). Doesn't that undermine this narrative you're trying to build of AI risk as being a religious concern spread by rationalists? Hinton clearly isn't getting this from having read _Harry Potter and the Methods of Rationality_.

**CM**: People get ...

**ZMD**: So are you proposing a model where like—

**CM**: Go back and read that article. Listen to [the _Daily_ episode I did about Geoff](https://www.nytimes.com/2023/05/30/podcasts/the-daily/chatgpt-hinton-ai.html). I push back on so much of what he says. I say in the articles, many of his students push back. They don't agree with it. So this is what needs to be conveyed to people, that many people working on the same technology, who are very smart, and very well educated, and very experienced, completely disagree on what is happening.

**ZMD**: Right. That part, I definitely agree that that part is part of the story.

**CM**: But there is also this community that has driven a lot of what is happening here, and people need to understand that.

**ZMD**: Yeah, but it just seems that the situation in terms of people who think AI is a big risk and people who think AI is just another technology, it seems to me that that debate is symmetrical, in the sense that you have smart people on both sides who just disagree about what's happening, and if you're specifically pointing at one side and saying, this is an argument based on faith, then that's editorializing, in a way that's not what you should be doing as a reporter just trying to convey the debate to readers who don't know that this is happening.

**CM**: I'm pointing out a key part of the debate. I'm pointing that out.
